{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94939,"databundleVersionId":11284612,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11096854,"sourceType":"datasetVersion","datasetId":6917490}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport pathlib as Path\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:20:41.657422Z","iopub.execute_input":"2025-03-22T06:20:41.657748Z","iopub.status.idle":"2025-03-22T06:20:53.245029Z","shell.execute_reply.started":"2025-03-22T06:20:41.657721Z","shell.execute_reply":"2025-03-22T06:20:53.244370Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# First I want to take the CelebA data set and sort it by Hat and No Hat and create a new dataset of sorted images\n### This code does not need to be run and will be commented out as dataset is already created as code takes significant time as it must sort 200K images\n#### Double # are comments","metadata":{}},{"cell_type":"code","source":"## Create directories for test images\n## One will hold images with Hat value of 1 and the other with hat value of -1 (No Hat)\n## Declare full paths\n# test_set_path = '/kaggle/working/test_set'\n# hat_set_path = '/kaggle/working/test_set/hat_set'\n# no_hat_set_path = '/kaggle/working/test_set/no_hat_set'\n\n## Create Directories using paths\n# os.makedirs(test_set_path, exist_ok=True)\n# os.makedirs(hat_set_path, exist_ok=True)\n# os.makedirs(no_hat_set_path, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:45:39.475246Z","iopub.execute_input":"2025-03-21T02:45:39.475548Z","iopub.status.idle":"2025-03-21T02:45:39.479797Z","shell.execute_reply.started":"2025-03-21T02:45:39.475522Z","shell.execute_reply":"2025-03-21T02:45:39.478446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Sort Images from celeba dataset into the newly created directories. \n## Will use the CSV to sort based on Has_hat to sort the images into the directories\n\n## Read the CSV that holds the attributes to each image\n# attributes = pd.read_csv(\"/kaggle/input/celeba-dataset/list_attr_celeba.csv\")\n\n## Path to the folder that holds all the images\n# image_path = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba'\n\n## Create a new dataframe that only has 2 columns, image ID and the hat attribute\n# test_images_df = attributes[['image_id', 'Wearing_Hat']]\n\n## Check size of the frame to ensure it looks correct by viewing how many of each attribute are present\n# print(test_images_df['Wearing_Hat'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T08:17:49.629733Z","iopub.execute_input":"2025-03-18T08:17:49.630204Z","iopub.status.idle":"2025-03-18T08:17:50.418124Z","shell.execute_reply.started":"2025-03-18T08:17:49.630171Z","shell.execute_reply":"2025-03-18T08:17:50.416988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n## Iterate through the dataframe using iterrows\n# for index, row in test_images_df.iterrows():\n    ## image name is set to the ID and wearing hat is set to the value of the hat attribute\n    # img_name = row['image_id']\n    # wearing_hat = row['Wearing_Hat']\n\n    ## Need the full path so taht the images can be sorted to respective folders\n    # full_path = os.path.join(image_path, img_name)\n\n    ## If the person is not wearing a hat the image destination is set to hat path\n    # if wearing_hat == -1: \n        # destination = os.path.join(no_hat_set_path, img_name)\n    ## If the person is wearing a hat path is set to hat path instead\n    # else:  \n        # destination = os.path.join(hat_set_path, img_name)\n\n    ## Using Shutil the image is then copied into each respective folder\n    ## This will allow images to be labeled as they are binary and sorted\n    # shutil.copy(full_path, destination)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T08:21:37.825995Z","iopub.execute_input":"2025-03-18T08:21:37.826389Z","iopub.status.idle":"2025-03-18T08:46:44.165402Z","shell.execute_reply.started":"2025-03-18T08:21:37.826359Z","shell.execute_reply":"2025-03-18T08:46:44.161447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n## Create zip files for both directories\n## I did this so that the files could be stored as a new dataset in Kaggle\n## Save time by not needing to repeat sort the images each time code is run\n# shutil.make_archive('/kaggle/working/test_set/hat_set', 'zip', hat_set_path)\n# shutil.make_archive('/kaggle/working/test_set/no_hat_set', 'zip', no_hat_set_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T08:18:31.366082Z","iopub.execute_input":"2025-03-18T08:18:31.366420Z","iopub.status.idle":"2025-03-18T08:18:31.374959Z","shell.execute_reply.started":"2025-03-18T08:18:31.366392Z","shell.execute_reply":"2025-03-18T08:18:31.373366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Test data ","metadata":{}},{"cell_type":"code","source":"# Need to create a data frame to process images to have access to IDs \ntest_image_folder = '/kaggle/input/hat-or-no-hat-that-is-the-question-spring-25/test_set/test_set'\n\n# Gather the paths for the folder\nfiles = os.listdir(test_image_folder)\n\n# Once the image paths are gathered, remove the extension (.jpg)\ntest_image_ids =  [os.path.splitext(file)[0] for file in files]\n# These are the IDs used for the submission\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:20:56.804857Z","iopub.execute_input":"2025-03-22T06:20:56.805365Z","iopub.status.idle":"2025-03-22T06:20:56.947623Z","shell.execute_reply.started":"2025-03-22T06:20:56.805342Z","shell.execute_reply":"2025-03-22T06:20:56.946682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a df for the test so that the images can be passed through a generator\nTest_df = pd.DataFrame(files, columns=['id'])\n\nTest_df['path'] = Test_df['id'].apply(lambda x: os.path.join(test_image_folder, x))\n\npath = Test_df.iat[60,1]\n\npath","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:21:00.480530Z","iopub.execute_input":"2025-03-22T06:21:00.480973Z","iopub.status.idle":"2025-03-22T06:21:00.497154Z","shell.execute_reply.started":"2025-03-22T06:21:00.480938Z","shell.execute_reply":"2025-03-22T06:21:00.496304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess Images","metadata":{}},{"cell_type":"code","source":"# Create a gen for the test data\ntest_gen = tf.keras.preprocessing.image.ImageDataGenerator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:21:03.714404Z","iopub.execute_input":"2025-03-22T06:21:03.714875Z","iopub.status.idle":"2025-03-22T06:21:03.781237Z","shell.execute_reply.started":"2025-03-22T06:21:03.714834Z","shell.execute_reply":"2025-03-22T06:21:03.780341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create training and validation data from the images that are provided\n\n# For training and validation, I realized that the directory name is test_set, HOWEVER, it should be train set\n# LEft this way to avoid having to rename entire set and reupload to Kaggle\n\ntraining_data = tf.keras.utils.image_dataset_from_directory(\n    '/kaggle/input/no-hat-and-hat/test_set', # Provide directory where images are stored\n    labels = \"inferred\", # Inferred labels = labels are based on folders\n    label_mode = \"binary\", # binary as we are dealing with yes or no (hat/no hat)\n    batch_size = 32,  # set the batch size\n    image_size = (224, 224), # Resize the images\n    shuffle = True, # Shuffle data to ensure randomness\n    seed = 42,  # Seed so it can be replicated\n    validation_split = .2, # create a validation split to test how well model learns\n    subset = \"training\", # Name of the subset\n)\n\nval_data = tf.keras.utils.image_dataset_from_directory(\n    '/kaggle/input/no-hat-and-hat/test_set',\n    labels = \"inferred\",\n    label_mode = \"binary\",\n    batch_size = 32,\n    image_size = (224, 224),\n    shuffle = True,\n    seed = 42, \n    validation_split = .2,\n    subset = \"validation\", \n)\n\ntest_data = test_gen.flow_from_dataframe(\n    dataframe= Test_df,\n    x_col ='path',\n    target_size = (224, 224),\n    color_mode=\"rgb\",\n    class_mode = None, \n    batch_size = 32,\n    shuffle = False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:21:05.927410Z","iopub.execute_input":"2025-03-22T06:21:05.927767Z","iopub.status.idle":"2025-03-22T06:21:34.583348Z","shell.execute_reply.started":"2025-03-22T06:21:05.927740Z","shell.execute_reply":"2025-03-22T06:21:34.582302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n# Get a batch from the training data\n# Want to visualize the images and see the labels to ensure that Hats and No hats are labeled correctly\nfor images, labels in training_data.take(2):  # take(1) gets only the first batch\n    # Only show first 5 images \n    for i in range(4):\n        plt.figure(figsize=(4,4))\n        plt.imshow(images[i].numpy().astype(\"uint8\")) # show images by converting to NP array\n        plt.title(labels[i].numpy()) # Label each image with respective label\n        plt.show() \n\n## Hats are labeled as 0 and No Hats are labeled as 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:21:37.071447Z","iopub.execute_input":"2025-03-22T06:21:37.071817Z","iopub.status.idle":"2025-03-22T06:21:38.944511Z","shell.execute_reply.started":"2025-03-22T06:21:37.071785Z","shell.execute_reply":"2025-03-22T06:21:38.943782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now we create the model ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Input, MaxPool2D, Flatten, Dense, Rescaling, Dropout,GlobalAveragePooling2D\nfrom tensorflow.keras.applications import ResNet50, ResNet101","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:21:46.347866Z","iopub.execute_input":"2025-03-22T06:21:46.348159Z","iopub.status.idle":"2025-03-22T06:21:46.358318Z","shell.execute_reply.started":"2025-03-22T06:21:46.348137Z","shell.execute_reply":"2025-03-22T06:21:46.357487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # Add Conv2d and Pool layers\n    #Conv2D(32, (3,3), activation ='relu'),\n    #MaxPool2D((2,2)),\n    #Conv2D(64, (3,3), activation = 'relu'),\n    #MaxPool2D((2,2)),\n    #Conv2D(128, (3,3), activation = 'relu'),\n    #MaxPool2D((2,2)),\n    #Conv2D(256, (3,3), activation = 'relu'),\n    #MaxPool2D((2,2)),\n    #Conv2D(512, (3, 3), activation='relu'),\n    #MaxPool2D((2, 2)),","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create an instance of the resnet model \nbase_model = tf.keras.applications.ResNet101(include_top = False,\n                                            weights = 'imagenet',\n                                           input_shape = (224, 224, 3),)\nfor layer in base_model.layers[-30:]:\n    layer.trainable = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:21:48.581452Z","iopub.execute_input":"2025-03-22T06:21:48.581790Z","iopub.status.idle":"2025-03-22T06:21:52.901778Z","shell.execute_reply.started":"2025-03-22T06:21:48.581761Z","shell.execute_reply":"2025-03-22T06:21:52.900820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_model = Sequential([\n    # Rescale the images to ensure normalization\n    Rescaling(1./255, input_shape = (224, 224,3)),\n    # Add the ResNet Model\n    base_model,\n    # Add flatten layer before Dense layers\n    GlobalAveragePooling2D(),\n    # Add Dense layers\n    Dense(256, activation = 'relu'),\n    Dropout(.4),\n    Dense(128, activation = 'relu'),\n    Dropout(.4),\n    # Binary classification means we want to use sigmoid\n    Dense(1, activation = 'sigmoid')\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:30:27.732303Z","iopub.execute_input":"2025-03-22T04:30:27.732661Z","iopub.status.idle":"2025-03-22T04:30:27.772989Z","shell.execute_reply.started":"2025-03-22T04:30:27.732636Z","shell.execute_reply":"2025-03-22T04:30:27.772097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check to make sure model is created properly\nresnet_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:30:32.314819Z","iopub.execute_input":"2025-03-22T04:30:32.315100Z","iopub.status.idle":"2025-03-22T04:30:32.338497Z","shell.execute_reply.started":"2025-03-22T04:30:32.315080Z","shell.execute_reply":"2025-03-22T04:30:32.337541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"# Compile model, use binary crossentropy (since output is binary)\nresnet_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics =['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:30:37.761215Z","iopub.execute_input":"2025-03-22T04:30:37.761540Z","iopub.status.idle":"2025-03-22T04:30:37.769707Z","shell.execute_reply.started":"2025-03-22T04:30:37.761514Z","shell.execute_reply":"2025-03-22T04:30:37.768918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model and run (train)\nresnet_model.fit(\n    training_data, \n    validation_data = val_data, \n    epochs =15, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:30:41.385956Z","iopub.execute_input":"2025-03-22T04:30:41.386282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# These are the IDs used for the submission\n#test_image_ids\n\n# Test Model using test data\npredictions = resnet_model.predict(test_data)\n\nprint(predictions[:10])  # Print the first 10 predictions\n\n## Hats are labeled as 0 and No Hats are labeled as 1\n# Need to create labels, so if the value of the prediction is >.5 round to 0 (Hat)\n# If greater than .5 (no Hat)\npredicted_class = [1 if pred >= .5 else 0 for pred in predictions]\n\n# if the value of class is 0 (Hat) if the value is 1 (No Hat)\npredicted_labels = ['Hat' if prcls == 0 else 'No Hat' for prcls in predicted_class]\n\n# Create data Frame for submission (id, prediction)\nsubmission_df = pd.DataFrame({\n    'id': test_image_ids,\n    'class': predicted_labels\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:00:24.892691Z","iopub.execute_input":"2025-03-22T04:00:24.892984Z","iopub.status.idle":"2025-03-22T04:00:55.645950Z","shell.execute_reply.started":"2025-03-22T04:00:24.892964Z","shell.execute_reply":"2025-03-22T04:00:55.645120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(submission_df['class'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T04:52:59.952490Z","iopub.execute_input":"2025-03-21T04:52:59.952830Z","iopub.status.idle":"2025-03-21T04:52:59.967715Z","shell.execute_reply.started":"2025-03-21T04:52:59.952772Z","shell.execute_reply":"2025-03-21T04:52:59.966551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save as CSV\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"submission saved as df\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T04:53:01.891620Z","iopub.execute_input":"2025-03-21T04:53:01.891936Z","iopub.status.idle":"2025-03-21T04:53:01.904943Z","shell.execute_reply.started":"2025-03-21T04:53:01.891913Z","shell.execute_reply":"2025-03-21T04:53:01.904042Z"}},"outputs":[],"execution_count":null}]}